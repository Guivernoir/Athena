use postprocessing::{
    context::TimestampInjector,
    formatter,
    interpreter,
    persona::PersonaApplier,
    templates::wrap_final,
    validator,
};

#[command]
pub async fn send_output(
    input: String,
    mode: u8,
    proficiency: u8,
    personality: u8,
    language: Language,
    llm: tauri::State<'_, Mutex<LLMEngine>>,
    state: tauri::State<'_, Mutex<AppState>>,
) -> Result<String, String> {
    // 1. Convert enums
    let mode_enum = Mode::select_mode(mode).await?;
    let prof_enum = Proficiency::select_proficiency(proficiency).await?;
    let pers_enum = Personality::select_personality(personality).await?;

    // 2. Pre-process
    let llm_guard = llm.lock().unwrap();
    let formatted = Preprocessor::process(
        input,
        mode_enum,
        prof_enum,
        pers_enum,
        language,
        &llm_guard,
    )
    .await
    .map_err(|e| e.to_string())?;

    // 3. Cache latest payload (for future retrieval)
    state.lock().unwrap().latest = Some(formatted.clone());

    // 4. **RAW LLM OUTPUT** ‚Üê placeholder 
    let raw_llm = "LLM reply placeholder";

    // 5. **POST-PROCESSING PIPELINE**
    let persona_filter = PersonaApplier {
        name: pers_enum.to_string(),
    };
    let timestamp_inj = TimestampInjector;

    let final_text = raw_llm
        .pipe(interpreter::interpret)
        .pipe(|s| validator::validate(s).map_err(|e| e.to_string()))?
        .pipe(|s| persona_filter.apply(&s))
        .pipe(|s| timestamp_inj.inject(&s))
        .pipe(formatter::clean)
        .pipe(wrap_final);

    Ok(final_text)
}