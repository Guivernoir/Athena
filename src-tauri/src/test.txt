#ifndef QWEN_ENGINE_HPP
#define QWEN_ENGINE_HPP

#ifdef __cplusplus
extern "C" {
#endif

/**
 * LLM Engine C Interface
 * 
 * Strategic deployment interface for llama.cpp integration.
 * Handle with appropriate tactical caution - pointers are live ammunition.
 */

void* qwen_engine_create(const char* model_path);
void qwen_engine_destroy(void* engine);
char* qwen_engine_generate(void* engine, const char* prompt, int max_tokens, float temperature);
char* qwen_engine_chat(void* engine, const char* system_prompt, const char* user_message, int max_tokens);
void qwen_free_string(char* str);
int qwen_engine_is_loaded(void* engine);
const char* qwen_engine_get_model_info(void* engine);

#ifdef __cplusplus
}
#endif

#endif // QWEN_ENGINE_HPP

use std::ffi::{CStr, CString};
use std::os::raw::{c_char, c_float, c_int, c_void};

/**
 * Raw FFI Bindings for LLM Engine
 * 
 * This is where we venture into the unsafe wilderness of C interop.
 * Handle with appropriate tactical caution - one wrong pointer dereference
 * and your entire operation goes sideways faster than a chess match with Kasparov.
 */
#[link(name = "llama")]
extern "C" {
    fn qwen_engine_create(model_path: *const c_char) -> *mut c_void;
    fn qwen_engine_destroy(engine: *mut c_void);
    fn qwen_engine_generate(
        engine: *mut c_void,
        prompt: *const c_char,
        max_tokens: c_int,
        temperature: c_float,
    ) -> *mut c_char;
    fn qwen_engine_chat(
        engine: *mut c_void,
        system_prompt: *const c_char,
        user_message: *const c_char,
        max_tokens: c_int,
    ) -> *mut c_char;
    fn qwen_free_string(str: *mut c_char);
    fn qwen_engine_is_loaded(engine: *mut c_void) -> c_int;
    fn qwen_engine_get_model_info(engine: *mut c_void) -> *const c_char;
}

pub struct RawEngine {
    pub(crate) ptr: *mut c_void,
}

impl RawEngine {
    pub unsafe fn new(model_path: &str) -> Option<Self> {
        let c_path = CString::new(model_path).ok()?;
        let ptr = qwen_engine_create(c_path.as_ptr());
        if ptr.is_null() {
            None
        } else {
            Some(RawEngine { ptr })
        }
    }
    pub unsafe fn generate(
        &self,
        prompt: &str,
        max_tokens: i32,
        temperature: f32,
    ) -> Option<String> {
        let c_prompt = CString::new(prompt).ok()?;
        let result_ptr = qwen_engine_generate(
            self.ptr,
            c_prompt.as_ptr(),
            max_tokens as c_int,
            temperature as c_float,
        );
        if result_ptr.is_null() {
            return None;
        }
        let c_str = CStr::from_ptr(result_ptr);
        let rust_string = c_str.to_string_lossy().into_owned();
        qwen_free_string(result_ptr);
        Some(rust_string)
    }
    pub unsafe fn chat(
        &self,
        system_prompt: Option<&str>,
        user_message: &str,
        max_tokens: i32,
    ) -> Option<String> {
        let c_user = CString::new(user_message).ok()?;
        let c_system = match system_prompt {
            Some(s) => Some(CString::new(s).ok()?),
            None => None,
        };
        let system_ptr = match &c_system {
            Some(s) => s.as_ptr(),
            None => std::ptr::null(),
        };
        let result_ptr = qwen_engine_chat(
            self.ptr,
            system_ptr,
            c_user.as_ptr(),
            max_tokens as c_int,
        );
        if result_ptr.is_null() {
            return None;
        }
        let c_str = CStr::from_ptr(result_ptr);
        let rust_string = c_str.to_string_lossy().into_owned();
        qwen_free_string(result_ptr);
        Some(rust_string)
    }
    pub unsafe fn is_loaded(&self) -> bool {
        qwen_engine_is_loaded(self.ptr) != 0
    }
    pub unsafe fn get_model_info(&self) -> String {
        let info_ptr = qwen_engine_get_model_info(self.ptr);
        if info_ptr.is_null() {
            return "Unknown model status".to_string();
        }
        let c_str = CStr::from_ptr(info_ptr);
        c_str.to_string_lossy().into_owned()
    }
}

impl Drop for RawEngine {
    fn drop(&mut self) {
        unsafe {
            if !self.ptr.is_null() {
                qwen_engine_destroy(self.ptr);
                self.ptr = std::ptr::null_mut();
            }
        }
    }
}

unsafe impl Send for RawEngine {}

pub(crate) mod utils {
    use super::*;
    pub fn to_c_string(s: &str) -> Result<CString, std::ffi::NulError> {
        CString::new(s)
    }
    pub unsafe fn from_c_string(ptr: *const c_char) -> Option<String> {
        if ptr.is_null() {
            return None;
        }
        let c_str = CStr::from_ptr(ptr);
        Some(c_str.to_string_lossy().into_owned())
    }
}

use std::path::Path;
use std::sync::{Arc, Mutex};
use thiserror::Error;

mod ffi;
use ffi::RawEngine;

/**
 * Safe Rust Wrapper for LLM Engine
 * 
 * This is your clean, safe interface that integrates seamlessly with your
 * existing Tauri backend. No unsafe blocks bleeding into your application logic,
 * no mysterious segfaults at 3 AM - just clean, idiomatic Rust that works.
 */

#[derive(Error, Debug)]
pub enum LLMError {
    #[error("Failed to initialize engine with model: {model_path}")]
    InitializationFailed { model_path: String },
    #[error("Engine is not loaded or has been disposed")]
    EngineNotLoaded,
    #[error("Text generation failed: {reason}")]
    GenerationFailed { reason: String },
    #[error("Model file not found: {path}")]
    ModelNotFound { path: String },
    #[error("Invalid input parameters: {details}")]
    InvalidInput { details: String },
}

pub type Result<T> = std::result::Result<T, LLMError>;

#[derive(Debug, Clone)]
pub struct GenerationConfig {
    pub max_tokens: i32,
    pub temperature: f32,
}

impl Default for GenerationConfig {
    fn default() -> Self {
        Self {
            max_tokens: 512,
            temperature: 0.7,
        }
    }
}

#[derive(Debug, Clone)]
pub struct ChatMessage {
    pub role: MessageRole,
    pub content: String,
}

#[derive(Debug, Clone)]
pub enum MessageRole {
    System,
    User,
    Assistant,
}

pub struct LLMEngine {
    inner: Arc<Mutex<Option<RawEngine>>>,
    model_path: String,
}

impl LLMEngine {
    pub fn new<P: AsRef<Path>>(model_path: P) -> Result<Self> {
        let path_str = model_path.as_ref().to_string_lossy().to_string();
        if !model_path.as_ref().exists() {
            return Err(LLMError::ModelNotFound { path: path_str });
        }
        let raw_engine = unsafe {
            RawEngine::new(&path_str).ok_or_else(|| {
                LLMError::InitializationFailed {
                    model_path: path_str.clone(),
                }
            })?
        };
        Ok(LLMEngine {
            inner: Arc::new(Mutex::new(Some(raw_engine))),
            model_path: path_str,
        })
    }
    pub fn generate(&self, prompt: &str, config: Option<GenerationConfig>) -> Result<String> {
        if prompt.trim().is_empty() {
            return Err(LLMError::InvalidInput {
                details: "Empty prompt provided".to_string(),
            });
        }
        let config = config.unwrap_or_default();
        let guard = self.inner.lock().unwrap();
        match guard.as_ref() {
            Some(engine) => {
                let result = unsafe {
                    engine.generate(prompt, config.max_tokens, config.temperature)
                };
                result.ok_or_else(|| LLMError::GenerationFailed {
                    reason: "C++ engine returned null result".to_string(),
                })
            }
            None => Err(LLMError::EngineNotLoaded),
        }
    }
    pub fn chat(
        &self,
        messages: &[ChatMessage],
        config: Option<GenerationConfig>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(LLMError::InvalidInput {
                details: "No messages provided".to_string(),
            });
        }
        let config = config.unwrap_or_default();
        let system_prompt = messages
            .iter()
            .find(|msg| matches!(msg.role, MessageRole::System))
            .map(|msg| msg.content.as_str());
        let user_message = messages
            .iter()
            .rev()
            .find(|msg| matches!(msg.role, MessageRole::User))
            .ok_or_else(|| LLMError::InvalidInput {
                details: "No user message found".to_string(),
            })?;
        let guard = self.inner.lock().unwrap();
        match guard.as_ref() {
            Some(engine) => {
                let result = unsafe {
                    engine.chat(system_prompt, &user_message.content, config.max_tokens)
                };
                result.ok_or_else(|| LLMError::GenerationFailed {
                    reason: "Chat generation failed".to_string(),
                })
            }
            None => Err(LLMError::EngineNotLoaded),
        }
    }
    pub fn simple_chat(
        &self,
        user_message: &str,
        system_prompt: Option<&str>,
        config: Option<GenerationConfig>,
    ) -> Result<String> {
        let mut messages = Vec::new();
        if let Some(sys) = system_prompt {
            messages.push(ChatMessage {
                role: MessageRole::System,
                content: sys.to_string(),
            });
        }
        messages.push(ChatMessage {
            role: MessageRole::User,
            content: user_message.to_string(),
        });
        self.chat(&messages, config)
    }
    pub fn is_loaded(&self) -> bool {
        let guard = self.inner.lock().unwrap();
        match guard.as_ref() {
            Some(engine) => unsafe { engine.is_loaded() },
            None => false,
        }
    }
    pub fn get_model_info(&self) -> String {
        let guard = self.inner.lock().unwrap();
        match guard.as_ref() {
            Some(engine) => unsafe { engine.get_model_info() },
            None => format!("Engine not loaded (model: {})", self.model_path),
        }
    }
    pub fn model_path(&self) -> &str {
        &self.model_path
    }
    pub fn dispose(&self) {
        let mut guard = self.inner.lock().unwrap();
        *guard = None;
    }
}

unsafe impl Send for LLMEngine {}
unsafe impl Sync for LLMEngine {}

impl Clone for LLMEngine {
    fn clone(&self) -> Self {
        LLMEngine {
            inner: Arc::clone(&self.inner),
            model_path: self.model_path.clone(),
        }
    }
}

impl LLMEngine {
    pub fn from_models_dir() -> Result<Self> {
        let model_path = "llama/models/qwen2.5-0.5b-instruct-q5_k_m.gguf";
        Self::new(model_path)
    }
    pub fn complete(&self, prompt: &str) -> Result<String> {
        self.generate(prompt, None)
    }
    pub fn assist(&self, user_message: &str) -> Result<String> {
        let system_prompt = "You are a helpful AI assistant. Provide clear, accurate, and concise responses.";
        self.simple_chat(user_message, Some(system_prompt), None)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_engine_creation() {
        let engine = LLMEngine::from_models_dir();
        assert!(engine.is_ok(), "Engine creation should succeed with valid model");
    }
    #[test]
    fn test_simple_generation() {
        let engine = LLMEngine::from_models_dir().unwrap();
        let result = engine.complete("Hello, world!");
        assert!(result.is_ok(), "Simple generation should work");
        println!("Generated: {}", result.unwrap());
    }
    
}