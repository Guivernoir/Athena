// quantization/kernels.cpp
#include "kernels.hpp"
#include <cstring>
#include <algorithm>
#include <cstdint>
#include <memory>

// SIMD headers - conditional compilation for maximum tactical flexibility
#ifdef HAVE_AVX2
#include <immintrin.h>
#elif defined(HAVE_SSE42)
#include <smmintrin.h>
#endif

// Platform-specific CPUID support
#ifdef _MSC_VER
#include <intrin.h>
#elif defined(__GNUC__) || defined(__clang__)
#include <cpuid.h>
// Define __cpuid for non-Windows platforms
inline void my_cpuid(int cpu_info[4], int info_type) {
    __get_cpuid(info_type, 
                reinterpret_cast<unsigned int*>(&cpu_info[0]),
                reinterpret_cast<unsigned int*>(&cpu_info[1]),
                reinterpret_cast<unsigned int*>(&cpu_info[2]),
                reinterpret_cast<unsigned int*>(&cpu_info[3]));
}
#else
// Provide a stub or error for unsupported compilers/platforms
#error "CPUID not supported on this platform/compiler."
#endif

int pack_bits(const uint8_t* input, int length, int bits_per_value, char* output) {
    if (bits_per_value == 8) {
        // Direct copy for 8-bit values
        std::memcpy(output, input, length);
        return length;
    }

    int output_bytes = (length * bits_per_value + 7) / 8;
    std::memset(output, 0, output_bytes);

    int bit_position = 0;
    
    for (int i = 0; i < length; ++i) {
        uint8_t value = input[i];
        
        // Ensure value fits in specified bits
        uint8_t mask = (1 << bits_per_value) - 1;
        value &= mask;
        
        int byte_index = bit_position / 8;
        int bit_offset = bit_position % 8;
        
        // Handle values that span multiple bytes
        if (bit_offset + bits_per_value <= 8) {
            // Value fits in current byte
            output[byte_index] |= (value << bit_offset);
        } else {
            // Value spans two bytes
            int bits_in_first_byte = 8 - bit_offset;
            int bits_in_second_byte = bits_per_value - bits_in_first_byte;
            
            output[byte_index] |= (value << bit_offset);
            output[byte_index + 1] |= (value >> bits_in_first_byte);
        }
        
        bit_position += bits_per_value;
    }
    
    return output_bytes;
}

int unpack_bits(const char* input, int length, int bits_per_value, uint8_t* output) {
    if (bits_per_value == 8) {
        // Direct copy for 8-bit values
        std::memcpy(output, input, length);
        return length;
    }

    int input_bits = length * 8;
    int num_values = input_bits / bits_per_value;
    uint8_t mask = (1 << bits_per_value) - 1;
    
    int bit_position = 0;
    
    for (int i = 0; i < num_values; ++i) {
        int byte_index = bit_position / 8;
        int bit_offset = bit_position % 8;
        
        uint8_t value = 0;
        
        if (bit_offset + bits_per_value <= 8) {
            // Value is contained in single byte
            value = (input[byte_index] >> bit_offset) & mask;
        } else {
            // Value spans two bytes
            int bits_in_first_byte = 8 - bit_offset;
            int bits_in_second_byte = bits_per_value - bits_in_first_byte;
            
            uint8_t first_part = (input[byte_index] >> bit_offset) & ((1 << bits_in_first_byte) - 1);
            uint8_t second_part = input[byte_index + 1] & ((1 << bits_in_second_byte) - 1);
            
            value = first_part | (second_part << bits_in_first_byte);
        }
        
        output[i] = value;
        bit_position += bits_per_value;
    }
    
    return num_values;
}

// Vectorized quantization - the real battlefield begins here
void quantize_block_simd(const float* input, int length, float scale, 
                        float zero_point, uint8_t* output) {
    
#ifdef HAVE_AVX2
    // AVX2 implementation - 8 floats at once, like a well-coordinated squad
    const __m256 scale_vec = _mm256_set1_ps(1.0f / scale);
    const __m256 zero_point_vec = _mm256_set1_ps(-zero_point / scale);
    const __m256 half_vec = _mm256_set1_ps(0.5f);
    const __m256i max_val = _mm256_set1_epi32(255);
    const __m256i zero_vec = _mm256_setzero_si256();
    
    int simd_length = (length / 8) * 8;
    
    for (int i = 0; i < simd_length; i += 8) {
        // Load 8 floats
        __m256 values = _mm256_load_ps(input + i);
        
        // Normalize: (input - zero_point) / scale
        values = _mm256_fmadd_ps(values, scale_vec, zero_point_vec);
        
        // Round to nearest integer
        values = _mm256_add_ps(values, half_vec);
        
        // Convert to integers
        __m256i int_vals = _mm256_cvtps_epi32(values);
        
        // Clamp to [0, 255] - tactical precision required
        int_vals = _mm256_max_epi32(int_vals, zero_vec);
        int_vals = _mm256_min_epi32(int_vals, max_val);
        
        // Pack to bytes using saturation (AVX2's secret weapon)
        __m256i packed_low = _mm256_packus_epi32(int_vals, int_vals);
        __m256i packed = _mm256_packus_epi16(packed_low, packed_low);
        
        // Extract and store - a bit of manual labor, but worth the performance
        uint64_t result = _mm256_extract_epi64(packed, 0);
        std::memcpy(output + i, &result, 8);
    }
    
    // Handle remaining elements with scalar precision
    for (int i = simd_length; i < length; ++i) {
        float normalized = (input[i] - zero_point) / scale;
        int quantized = static_cast<int>(normalized + 0.5f);
        quantized = std::max(0, std::min(quantized, 255));
        output[i] = static_cast<uint8_t>(quantized);
    }
    
#elif defined(HAVE_SSE42)
    // SSE4.2 implementation - 4 floats at once, still respectable firepower
    const __m128 scale_vec = _mm_set1_ps(1.0f / scale);
    const __m128 zero_point_vec = _mm_set1_ps(-zero_point / scale);
    const __m128 half_vec = _mm_set1_ps(0.5f);
    const __m128i max_val = _mm_set1_epi32(255);
    const __m128i zero_vec = _mm_setzero_si128();
    
    int simd_length = (length / 4) * 4;
    
    for (int i = 0; i < simd_length; i += 4) {
        // Load 4 floats
        __m128 values = _mm_load_ps(input + i);
        
        // Normalize
        values = _mm_add_ps(_mm_mul_ps(values, scale_vec), zero_point_vec);
        
        // Round to nearest
        values = _mm_add_ps(values, half_vec);
        
        // Convert to integers
        __m128i int_vals = _mm_cvtps_epi32(values);
        
        // Clamp to [0, 255]
        int_vals = _mm_max_epi32(int_vals, zero_vec);
        int_vals = _mm_min_epi32(int_vals, max_val);
        
        // Pack to bytes
        __m128i packed_low = _mm_packus_epi32(int_vals, int_vals);
        __m128i packed = _mm_packus_epi16(packed_low, packed_low);
        
        // Extract and store
        uint32_t result = _mm_extract_epi32(packed, 0);
        std::memcpy(output + i, &result, 4);
    }
    
    // Handle remaining elements
    for (int i = simd_length; i < length; ++i) {
        float normalized = (input[i] - zero_point) / scale;
        int quantized = static_cast<int>(normalized + 0.5f);
        quantized = std::max(0, std::min(quantized, 255));
        output[i] = static_cast<uint8_t>(quantized);
    }
    
#else
    // Scalar fallback - sometimes you fight with what you have
    for (int i = 0; i < length; ++i) {
        float normalized = (input[i] - zero_point) / scale;
        int quantized = static_cast<int>(normalized + 0.5f);
        quantized = std::max(0, std::min(quantized, 255));
        output[i] = static_cast<uint8_t>(quantized);
    }
#endif
}

void dequantize_block_simd(const uint8_t* input, int length, float scale, 
                          float zero_point, float* output) {
    
#ifdef HAVE_AVX2
    // AVX2 dequantization - elegant in its mathematical precision
    const __m256 scale_vec = _mm256_set1_ps(scale);
    const __m256 zero_point_vec = _mm256_set1_ps(zero_point);
    
    int simd_length = (length / 8) * 8;
    
    for (int i = 0; i < simd_length; i += 8) {
        // Load 8 bytes and convert to 32-bit integers
        uint64_t bytes;
        std::memcpy(&bytes, input + i, 8);
        
        __m128i bytes_128 = _mm_set1_epi64x(bytes);
        __m256i bytes_256 = _mm256_cvtepu8_epi32(bytes_128);
        
        // Convert to floats
        __m256 float_vals = _mm256_cvtepi32_ps(bytes_256);
        
        // Dequantize: quantized * scale + zero_point
        __m256 result = _mm256_fmadd_ps(float_vals, scale_vec, zero_point_vec);
        
        // Store result
        _mm256_store_ps(output + i, result);
    }
    
    // Handle remaining elements
    for (int i = simd_length; i < length; ++i) {
        output[i] = input[i] * scale + zero_point;
    }
    
#elif defined(HAVE_SSE42)
    // SSE4.2 dequantization
    const __m128 scale_vec = _mm_set1_ps(scale);
    const __m128 zero_point_vec = _mm_set1_ps(zero_point);
    
    int simd_length = (length / 4) * 4;
    
    for (int i = 0; i < simd_length; i += 4) {
        // Load 4 bytes and convert to 32-bit integers
        uint32_t bytes;
        std::memcpy(&bytes, input + i, 4);
        
        __m128i bytes_128 = _mm_set1_epi32(bytes);
        __m128i int_vals = _mm_cvtepu8_epi32(bytes_128);
        
        // Convert to floats
        __m128 float_vals = _mm_cvtepi32_ps(int_vals);
        
        // Dequantize
        __m128 result = _mm_add_ps(_mm_mul_ps(float_vals, scale_vec), zero_point_vec);
        
        // Store result
        _mm_store_ps(output + i, result);
    }
    
    // Handle remaining elements
    for (int i = simd_length; i < length; ++i) {
        output[i] = input[i] * scale + zero_point;
    }
    
#else
    // Scalar fallback
    for (int i = 0; i < length; ++i) {
        output[i] = input[i] * scale + zero_point;
    }
#endif
}

// Utility functions for performance analysis
float calculate_compression_ratio(int original_bits, int compressed_bits, int length) {
    return static_cast<float>(original_bits * length) / (compressed_bits * length);
}

void benchmark_quantization(const float* input, int length, int iterations) {
    // This would be useful for tactical performance assessment
    // Implementation left as an exercise for the strategically minded
}

// SIMD capability detection - intelligence gathering at its finest
bool has_avx2_support() {
#ifdef HAVE_AVX2
    // Check CPUID for AVX2 support
    int cpu_info[4];
    my_cpuid(cpu_info, 7);
    return (cpu_info[1] & (1 << 5)) != 0; // EBX bit 5 indicates AVX2
#else
    return false;
#endif
}

bool has_sse42_support() {
#ifdef HAVE_SSE42
    // Check CPUID for SSE4.2 support
    int cpu_info[4];
    my_cpuid(cpu_info, 1);
    return (cpu_info[2] & (1 << 20)) != 0; // ECX bit 20 indicates SSE4.2
#else
    return false;
#endif
}

// quantization/kernels.hpp 
#ifndef KERNELS_HPP
#define KERNELS_HPP

#include <cstdint>

// Efficient bit packing/unpacking operations
int pack_bits(const uint8_t* input, int length, int bits_per_value, char* output);
int unpack_bits(const char* input, int length, int bits_per_value, uint8_t* output);

// SIMD-optimized kernels (AVX2/SSE4.2 when available, scalar fallback otherwise)
void quantize_block_simd(const float* input, int length, float scale, 
                        float zero_point, uint8_t* output);
void dequantize_block_simd(const uint8_t* input, int length, float scale, 
                          float zero_point, float* output);

// Performance analysis utilities
float calculate_compression_ratio(int original_bits, int compressed_bits, int length);
void benchmark_quantization(const float* input, int length, int iterations);

// Utility functions
inline int calculate_packed_size(int num_values, int bits_per_value) {
    return (num_values * bits_per_value + 7) / 8;
}

inline int calculate_unpacked_size(int packed_bytes, int bits_per_value) {
    return (packed_bytes * 8) / bits_per_value;
}

// Memory alignment helpers for SIMD operations
inline bool is_aligned(const void* ptr, size_t alignment) {
    return reinterpret_cast<uintptr_t>(ptr) % alignment == 0;
}

template<typename T>
inline T* align_pointer(T* ptr, size_t alignment) {
    uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);
    uintptr_t aligned = (addr + alignment - 1) & ~(alignment - 1);
    return reinterpret_cast<T*>(aligned);
}

// SIMD capability detection at runtime
bool has_avx2_support();
bool has_sse42_support();

#endif

// quantization/quantizer.cpp 
#include "quantizer.hpp"
#include "kernels.hpp"
#include <algorithm>
#include <cmath>
#include <memory>
#include <vector>

class Quantizer {
private:
    int bits_;
    int block_size_;
    std::vector<float> scales_;
    std::vector<float> zero_points_;

public:
    Quantizer(int bits, int block_size) 
        : bits_(bits), block_size_(block_size) {
        if (bits <= 0 || bits > 16) {
            throw std::invalid_argument("Bits must be between 1 and 16");
        }
        if (block_size <= 0) {
            throw std::invalid_argument("Block size must be positive");
        }
    }

    bool quantize_weights(const float* weights, int length, char* output) {
        try {
            // Calculate number of blocks
            int num_blocks = (length + block_size_ - 1) / block_size_;
            scales_.resize(num_blocks);
            zero_points_.resize(num_blocks);

            // Calculate scales and zero points for each block
            for (int block = 0; block < num_blocks; ++block) {
                int start_idx = block * block_size_;
                int end_idx = std::min(start_idx + block_size_, length);
                
                calculate_scale_and_zero_point(
                    weights + start_idx, 
                    end_idx - start_idx, 
                    scales_[block], 
                    zero_points_[block]
                );
            }

            // Quantize each block
            int output_offset = 0;
            for (int block = 0; block < num_blocks; ++block) {
                int start_idx = block * block_size_;
                int end_idx = std::min(start_idx + block_size_, length);
                int block_length = end_idx - start_idx;

                output_offset += quantize_block(
                    weights + start_idx,
                    block_length,
                    scales_[block],
                    zero_points_[block],
                    output + output_offset
                );
            }

            return true;
        } catch (const std::exception& e) {
            return false;
        }
    }

    bool dequantize_weights(const char* quantized, int length, float* output) {
        try {
            int num_blocks = scales_.size();
            int input_offset = 0;
            
            for (int block = 0; block < num_blocks; ++block) {
                int block_length = std::min(block_size_, 
                    static_cast<int>(scales_.size()) * block_size_ - block * block_size_);
                
                input_offset += dequantize_block(
                    quantized + input_offset,
                    block_length,
                    scales_[block],
                    zero_points_[block],
                    output + block * block_size_
                );
            }

            return true;
        } catch (const std::exception& e) {
            return false;
        }
    }

private:
    void calculate_scale_and_zero_point(const float* data, int length, 
                                       float& scale, float& zero_point) {
        if (length == 0) {
            scale = 1.0f;
            zero_point = 0.0f;
            return;
        }

        // Find min and max values
        float min_val = data[0];
        float max_val = data[0];
        
        for (int i = 1; i < length; ++i) {
            min_val = std::min(min_val, data[i]);
            max_val = std::max(max_val, data[i]);
        }

        // Calculate quantization parameters
        int max_quantized = (1 << bits_) - 1;
        
        if (max_val == min_val) {
            scale = 1.0f;
            zero_point = 0.0f;
        } else {
            scale = (max_val - min_val) / max_quantized;
            zero_point = min_val;
        }
    }

    int quantize_block(const float* input, int length, float scale, 
                      float zero_point, char* output) {
        std::vector<uint8_t> quantized_values(length);
        
        // Quantize values
        for (int i = 0; i < length; ++i) {
            float normalized = (input[i] - zero_point) / scale;
            int quantized = static_cast<int>(std::round(normalized));
            quantized = std::max(0, std::min(quantized, (1 << bits_) - 1));
            quantized_values[i] = static_cast<uint8_t>(quantized);
        }

        // Pack bits efficiently
        return pack_bits(quantized_values.data(), length, bits_, output);
    }

    int dequantize_block(const char* input, int length, float scale, 
                        float zero_point, float* output) {
        std::vector<uint8_t> quantized_values(length);
        
        // Unpack bits
        int bytes_read = unpack_bits(input, length, bits_, quantized_values.data());
        
        // Dequantize values
        for (int i = 0; i < length; ++i) {
            output[i] = quantized_values[i] * scale + zero_point;
        }

        return bytes_read;
    }
};

// C interface functions
extern "C" {
    void* create_quantizer(int bits, int block_size) {
        try {
            return new Quantizer(bits, block_size);
        } catch (...) {
            return nullptr;
        }
    }

    void destroy_quantizer(void* quantizer) {
        delete static_cast<Quantizer*>(quantizer);
    }

    int quantize_weights(void* quantizer, const float* weights, 
                        int length, char* output) {
        if (!quantizer) return 0;
        
        Quantizer* q = static_cast<Quantizer*>(quantizer);
        return q->quantize_weights(weights, length, output) ? 1 : 0;
    }

    int dequantize_weights(void* quantizer, const char* quantized, 
                          int length, float* output) {
        if (!quantizer) return 0;
        
        Quantizer* q = static_cast<Quantizer*>(quantizer);
        return q->dequantize_weights(quantized, length, output) ? 1 : 0;
    }
}

// quantization/quantizer.hpp 
#ifndef QUANTIZER_HPP
#define QUANTIZER_HPP

#ifdef __cplusplus
extern "C" {
#endif

// C interface for Rust FFI
void* create_quantizer(int bits, int block_size);
void destroy_quantizer(void* quantizer);
int quantize_weights(void* quantizer, const float* weights, int length, char* output);
int dequantize_weights(void* quantizer, const char* quantized, int length, float* output);

#ifdef __cplusplus
}
#endif

#endif

// quantization/bridge.rs 
use crate::quantization::{QuantizationConfig, QuantizationError, Result};
use std::sync::Arc;
use std::collections::HashMap;

/// High-level bridge for quantization operations
pub struct QuantizationBridge {
    configs: HashMap<String, QuantizationConfig>,
    active_quantizers: HashMap<String, Arc<super::Quantizer>>,
}

impl QuantizationBridge {
    pub fn new() -> Self {
        Self {
            configs: HashMap::new(),
            active_quantizers: HashMap::new(),
        }
    }

    /// Register a quantization configuration
    pub fn register_config(&mut self, name: String, config: QuantizationConfig) -> Result<()> {
        config.validate().map_err(|_| QuantizationError::InvalidBitWidth)?;
        self.configs.insert(name, config);
        Ok(())
    }

    /// Create a quantizer from registered config
    pub fn create_quantizer(&mut self, config_name: &str) -> Result<()> {
        let config = self.configs.get(config_name)
            .ok_or(QuantizationError::InvalidBitWidth)?
            .clone();

        let quantizer = super::Quantizer::new(config)?;
        self.active_quantizers.insert(config_name.to_string(), Arc::new(quantizer));
        Ok(())
    }

    /// Quantize weights using named quantizer
    pub fn quantize_with_name(&self, name: &str, weights: &[f32]) -> Result<Vec<u8>> {
        let quantizer = self.active_quantizers.get(name)
            .ok_or(QuantizationError::NullPointer)?;
        quantizer.quantize(weights)
    }

    /// Dequantize weights using named quantizer
    pub fn dequantize_with_name(&self, name: &str, quantized: &[u8]) -> Result<Vec<f32>> {
        let quantizer = self.active_quantizers.get(name)
            .ok_or(QuantizationError::NullPointer)?;
        quantizer.dequantize(quantized)
    }

    /// Batch quantization for multiple weight tensors
    pub fn batch_quantize(&self, name: &str, weight_batches: &[&[f32]]) -> Result<Vec<Vec<u8>>> {
        let quantizer = self.active_quantizers.get(name)
            .ok_or(QuantizationError::NullPointer)?;

        let mut results = Vec::with_capacity(weight_batches.len());
        for weights in weight_batches {
            results.push(quantizer.quantize(weights)?);
        }
        Ok(results)
    }

    /// Get compression statistics
    pub fn get_compression_stats(&self, name: &str, original_size: usize) -> Option<CompressionStats> {
        let config = self.configs.get(name)?;
        let compressed_size = (original_size * config.bits + 7) / 8;
        
        Some(CompressionStats {
            original_size,
            compressed_size,
            compression_ratio: config.compression_ratio(),
            space_saved: original_size - compressed_size,
        })
    }

    /// List all registered configurations
    pub fn list_configs(&self) -> Vec<String> {
        self.configs.keys().cloned().collect()
    }

    /// Remove a quantizer
    pub fn remove_quantizer(&mut self, name: &str) -> bool {
        self.active_quantizers.remove(name).is_some()
    }
}

#[derive(Debug, Clone)]
pub struct CompressionStats {
    pub original_size: usize,
    pub compressed_size: usize,
    pub compression_ratio: f32,
    pub space_saved: usize,
}

impl Default for QuantizationBridge {
    fn default() -> Self {
        let mut bridge = Self::new();
        
        // Register common configurations
        let _ = bridge.register_config("int4".to_string(), QuantizationConfig::int4());
        let _ = bridge.register_config("int8".to_string(), QuantizationConfig::int8());
        let _ = bridge.register_config("int16".to_string(), QuantizationConfig::int16());
        
        bridge
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_bridge_creation() {
        let bridge = QuantizationBridge::new();
        assert_eq!(bridge.configs.len(), 0);
    }

    #[test]
    fn test_config_registration() {
        let mut bridge = QuantizationBridge::new();
        let config = QuantizationConfig::int8();
        
        assert!(bridge.register_config("test".to_string(), config).is_ok());
        assert_eq!(bridge.configs.len(), 1);
    }

    #[test]
    fn test_default_configs() {
        let bridge = QuantizationBridge::default();
        let configs = bridge.list_configs();
        
        assert!(configs.contains(&"int4".to_string()));
        assert!(configs.contains(&"int8".to_string()));
        assert!(configs.contains(&"int16".to_string()));
    }
}

// quantization/config.rs 
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QuantizationConfig {
    /// Number of bits for quantization (typically 4, 8, or 16)
    pub bits: usize,
    /// Block size for grouped quantization
    pub block_size: usize,
    /// Quantization method
    pub method: QuantizationMethod,
    /// Whether to use symmetric quantization
    pub symmetric: bool,
    /// Calibration dataset size (for dynamic quantization)
    pub calibration_samples: Option<usize>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QuantizationMethod {
    /// Simple linear quantization
    Linear,
    /// Dynamic range quantization
    Dynamic,
    /// Block-wise quantization with shared scales
    BlockWise,
    /// K-means clustering quantization
    KMeans,
}

impl Default for QuantizationConfig {
    fn default() -> Self {
        Self {
            bits: 8,
            block_size: 128,
            method: QuantizationMethod::BlockWise,
            symmetric: true,
            calibration_samples: Some(1000),
        }
    }
}

impl QuantizationConfig {
    /// Create a new config with validation
    pub fn new(bits: usize, block_size: usize) -> Result<Self, String> {
        if bits == 0 || bits > 16 {
            return Err("Bit width must be between 1 and 16".to_string());
        }
        
        if block_size == 0 || block_size > 1024 {
            return Err("Block size must be between 1 and 1024".to_string());
        }

        Ok(Self {
            bits,
            block_size,
            ..Default::default()
        })
    }

    /// Preset for 4-bit quantization (aggressive compression)
    pub fn int4() -> Self {
        Self {
            bits: 4,
            block_size: 64,
            method: QuantizationMethod::BlockWise,
            symmetric: true,
            calibration_samples: Some(2000),
        }
    }

    /// Preset for 8-bit quantization (balanced)
    pub fn int8() -> Self {
        Self {
            bits: 8,
            block_size: 128,
            method: QuantizationMethod::Dynamic,
            symmetric: false,
            calibration_samples: Some(1000),
        }
    }

    /// Preset for 16-bit quantization (high precision)
    pub fn int16() -> Self {
        Self {
            bits: 16,
            block_size: 256,
            method: QuantizationMethod::Linear,
            symmetric: true,
            calibration_samples: None,
        }
    }

    /// Calculate compression ratio
    pub fn compression_ratio(&self) -> f32 {
        32.0 / (self.bits as f32)
    }

    /// Validate configuration
    pub fn validate(&self) -> Result<(), String> {
        if self.bits == 0 || self.bits > 16 {
            return Err("Invalid bit width".to_string());
        }

        if self.block_size == 0 {
            return Err("Invalid block size".to_string());
        }

        if let Some(samples) = self.calibration_samples {
            if samples == 0 {
                return Err("Calibration samples must be > 0".to_string());
            }
        }

        Ok(())
    }
}

// quantization/mod.rs 
pub mod bridge;
pub mod config;

use std::ffi::CString;
use std::os::raw::{c_char, c_float, c_int, c_void};

// External C++ function declarations
extern "C" {
    fn create_quantizer(bits: c_int, block_size: c_int) -> *mut c_void;
    fn destroy_quantizer(quantizer: *mut c_void);
    fn quantize_weights(
        quantizer: *mut c_void,
        weights: *const c_float,
        length: c_int,
        output: *mut c_char,
    ) -> c_int;
    fn dequantize_weights(
        quantizer: *mut c_void,
        quantized: *const c_char,
        length: c_int,
        output: *mut c_float,
    ) -> c_int;
}

pub use bridge::QuantizationBridge;
pub use config::QuantizationConfig;

#[derive(Debug)]
pub enum QuantizationError {
    InvalidBitWidth,
    InvalidBlockSize,
    QuantizationFailed,
    DequantizationFailed,
    NullPointer,
}

pub type Result<T> = std::result::Result<T, QuantizationError>;

// Main quantization interface
pub struct Quantizer {
    ptr: *mut c_void,
    config: QuantizationConfig,
}

impl Quantizer {
    pub fn new(config: QuantizationConfig) -> Result<Self> {
        let ptr = unsafe { 
            create_quantizer(config.bits as c_int, config.block_size as c_int) 
        };
        
        if ptr.is_null() {
            return Err(QuantizationError::NullPointer);
        }

        Ok(Quantizer { ptr, config })
    }

    pub fn quantize(&self, weights: &[f32]) -> Result<Vec<u8>> {
        let mut output = vec![0u8; self.calculate_output_size(weights.len())];
        
        let result = unsafe {
            quantize_weights(
                self.ptr,
                weights.as_ptr(),
                weights.len() as c_int,
                output.as_mut_ptr() as *mut c_char,
            )
        };

        if result == 0 {
            Err(QuantizationError::QuantizationFailed)
        } else {
            Ok(output)
        }
    }

    pub fn dequantize(&self, quantized: &[u8]) -> Result<Vec<f32>> {
        let output_size = self.calculate_dequant_size(quantized.len());
        let mut output = vec![0.0f32; output_size];

        let result = unsafe {
            dequantize_weights(
                self.ptr,
                quantized.as_ptr() as *const c_char,
                quantized.len() as c_int,
                output.as_mut_ptr(),
            )
        };

        if result == 0 {
            Err(QuantizationError::DequantizationFailed)
        } else {
            Ok(output)
        }
    }

    fn calculate_output_size(&self, input_len: usize) -> usize {
        // Strategic calculation: bits per weight * number of weights / 8 bits per byte
        (input_len * self.config.bits + 7) / 8
    }

    fn calculate_dequant_size(&self, quantized_len: usize) -> usize {
        // Reverse calculation with proper rounding
        (quantized_len * 8) / self.config.bits
    }
}

impl Drop for Quantizer {
    fn drop(&mut self) {
        if !self.ptr.is_null() {
            unsafe { destroy_quantizer(self.ptr) };
        }
    }
}

unsafe impl Send for Quantizer {}
unsafe impl Sync for Quantizer {}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_quantizer_creation() {
        let config = QuantizationConfig::default();
        let quantizer = Quantizer::new(config);
        assert!(quantizer.is_ok());
    }

    #[test]
    fn test_quantization_roundtrip() {
        let config = QuantizationConfig::default();
        let quantizer = Quantizer::new(config).unwrap();
        
        let weights = vec![1.0, -0.5, 0.25, -0.125];
        let quantized = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized).unwrap();
        
        // Allow for quantization precision loss
        for (original, recovered) in weights.iter().zip(dequantized.iter()) {
            assert!((original - recovered).abs() < 0.1);
        }
    }
}